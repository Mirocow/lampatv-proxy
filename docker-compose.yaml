version: "3.8"

services:
  lampa-proxy:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    # environment:
    #   - PYTHONPATH=/app/src
    # env_file:
    #   - .env
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  lampa-proxy-tests:
    build:
      context: .
      dockerfile: Dockerfile.test
    # -v --cov=src.implementations.request_processor --cov-report=term-missing
    command: >
      sh -c "
        python -m pytest tests/ -v --tb=short --cov=src --cov-report=term-missing --cov-report=html --asyncio-mode=auto -W ignore::DeprecationWarning;
      "
    ports:
      - "5678:5678"
    environment:
      # - PYTHONPATH=/app/src
      - LOG_LEVEL=ERROR
      - USE_PROXY=false
      - PROXY_TEST_URL=http://httpbin.org/ip
      - PROXY_TEST_TIMEOUT=5
      - MAX_PROXY_RETRIES=2
      - MAX_REDIRECTS=3
      - STREAM_CHUNK_SIZE=8192
      - STREAM_TIMEOUT=30
      - TIMEOUT_CONNECT=5
      - TIMEOUT_READ=10
      - TIMEOUT_WRITE=5
      - TIMEOUT_POOL=5
      - MAX_RANGE_SIZE=10485760
      - MAX_REQUEST_SIZE=1048576
      - HEAD_REQUEST_TIMEOUT=10
      - PYDEVD_DISABLE_FILE_VALIDATION=1
    # env_file:
    #   - .env
    volumes:
      - ./src:/app/src:ro
      - ./tests:/app/tests:ro
      - ./htmlcov:/app/htmlcov
    stdin_open: true
    tty: true

  lampa-proxy-debug:
    build:
      context: .
      dockerfile: Dockerfile.dev
    command: >
      sh -c "python -m debugpy --listen 0.0.0.0:5678 --wait-for-client -m uvicorn src.main:app --host 0.0.0.0 --port 8080"
    # environment:
    #   - PYTHONPATH=/app/src
    # env_file:
    #   - .env
    ports:
      - "8080:8080"
      - "5678:5678"
    volumes:
      - ./src:/app/src:ro
    stdin_open: true
    tty: true
